\documentclass[letterpaper]{article}
% Used to do math
\usepackage{amsmath}
\usepackage{amssymb}

% Used to color text (for todos)
\usepackage{xcolor}
% for when defining a new command to have to include a space at the end
\usepackage{xspace}
% for double space support
\usepackage{setspace}
\usepackage{mathrsfs}

% Used to embed pdfs from yEd and other sources
\usepackage{graphicx}
% Used to embed gnuplot output into document
\usepackage{epstopdf}
\usepackage{pdflscape}
\usepackage{geometry}

% For figures
\usepackage{float}
\usepackage{subfig}
\usepackage{wrapfig}

% packages for doing elaborate table stuff
\usepackage{array}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
% Used to have a table span multiple pages.
\usepackage{longtable}

% For writing pseudo code
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% for code listings
\usepackage{listingsutf8}

% Deal with backwards quotes because evidently Latex doesn't know better.
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\newcommand{\TODO}[1]{\textcolor{red}{TODO: #1}}

\newcommand{\argmax}[1]{\underset{#1}{\text{argmax }}}
\newcommand{\cprob}[2]{ \prob{#1 \lvert #2} }
\newcommand{\prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\partialDx}[2]{\frac{\partial #1}{ \partial #2  }}
\newcommand{\inAngle}[1]{\left \langle #1 \right \rangle}

\newgeometry{margin=1.125in}

\begin{document}

\title{Deep Learning for Automatic Speech Recognition}
\author{Ryan Hartsfield \and Garrett Lewellen}
\date{May 4, 2015}

\maketitle

\doublespacing

\section*{Introduction}

\paragraph{} In this paper we examine two deep learning method for automatic speech recognition: deep neural networks and convolutional neural networks based on the works of \cite{DBLP:journals/taslp/DahlYDA12} and  \cite{DBLP:journals/taslp/Abdel-HamidMJDPY14} respectively. \TODO{Ryan TODO}

\section*{Traditional Approach} \TODO{Ryan TODO}

\section*{Deep Neural Networks Approach} 

\paragraph{} In this section we discuss the work on Large-Vocabulary Speech Recognition (LVSR) of \cite{DBLP:journals/taslp/DahlYDA12} consisting of a context dependent hidden markov model and deep neural network hybrid architecture (CD-HMM-DNN) for the acoustic model. We will begin with an overview of the general architecture, then explain the algorithms used for pre-training, and conclude a discussion on the general procedure for training. Discussion of experimental results for this approach are deferred to the results section so that they can be compared to the convolutional neural network approach.

\subsection*{Architecture}

\TODO{Include figure of architecture?}

\paragraph{} To motivate their architecture, \cite{DBLP:journals/taslp/DahlYDA12} rely on the standard noisy channel model for speech recognition presented in \cite{jurafskyMartin} where we wish maximize the likelihood of a decoded word sequence given our input audio observations:

\begin{equation}
\hat{w} = \argmax{w \in \mathscr{L}} \cprob{w}{x} = \argmax{w \in \mathscr{L}} \cprob{x}{w} \prob{w} 
\label{eqn:asr:def}
\end{equation}

Where $\prob{w}$ and $\cprob{x}{w}$ represent the language and acoustic models respectively. \cite{jurafskyMartin} state that the language model can be computed via an N-gram approach, but \cite{DBLP:journals/taslp/DahlYDA12} do not state their method, instead the authors put their efforts into explaining their acoustic model:

\begin{equation}
\cprob{x}{w} = \sum_{q} \cprob{x,q}{w} \cprob{q}{w} \approxeq \max \pi(q_0) \prod_{t = 1}^T a_{q_{t-1} q_t} \prod_{t=0}^T \cprob{x_t}{q_t} 
\label{eqn:lm:def}
\end{equation}

Here the acoustic model is viewed as a sequence of transitions between states of tied-state triphones which \cite{DBLP:journals/taslp/DahlYDA12} refer to as senones \TODO{EXPLAIN WHAT SENONES ARE AND WHERE THEY COME FROM AND WHY THEY ARE USEFUL. \S 10.3 of \cite{jurafskyMartin} discusses Context-dependent Acoustic models: triphones and \cite{DBLP:conf/interspeech/2014} talks about senone (tied-state triphones)} which gives us the context dependent aspect of the architecture. The model assumes that there is a probability $\pi(q_0)$ for the starting state, probabilities $a_{q_{t-1} q_{t}}$ of transitioning to the state observed at step $t -1$ to step $t$, and finally, the probability of the acoustics given the current state $q_t$. \cite{DBLP:journals/taslp/DahlYDA12} expand this last term further into:

\begin{equation*}
	\cprob{x_t}{q_t} = \frac{\cprob{q_t}{x_t} \prob{x_t}}{\prob{q_t}}
	\label{eqn:senone:def}	
\end{equation*}

Where $\cprob{x_t}{q_t}$ models the tied triphone senone posterior given mel-frequency cepstral coefficients (MFCCs) based on 11 sampled frames of audio \TODO{EXPLAIN MFCCs AND WHY THEY ARE USEFUL; Jurasky seems to have a good explanation, may need to consult other resources}, $\prob{q_t}$ is the prior probability of the senone, and $\prob{x_t}$ can be ignored since it does not vary based on the decoded word sequence we are trying to find.

\paragraph{} Based on this formalism, \cite{DBLP:journals/taslp/DahlYDA12} chose to use pre-trained deep neural networks to estimate $\cprob{q_t}{x_t}$ using MFCCs as DNN inputs and taking the senone posterior probabilities as DNN outputs. The transitioning between events is bested modeled by hidden markov models whose notation, $\pi, a$, and $q$ appears in Eqn. (\ref{eqn:lm:def}). Now that we have an overview of the general CD-DNN-HMM architecture, we can look at how \cite{DBLP:journals/taslp/DahlYDA12} train their model.

\subsection*{Pre-Training}

\paragraph{} Given the DNN model we wish to fit the parameters of the model to a training set. This is usually accomplished by minimizing a likelihood function and deploying a gradient descent procedure to update the weights. One complication to this approach is that the likelihood can be computationally expensive for multilayer networks with many nodes rendering the approach unusable. As an alternative, one can attempt to optimize a computationally tractable surrogate to the likelihood. In this case the surrogate is the contrastive divergence developed by \cite{DBLP:journals/neco/Hinton02}. This sidestep enabled \cite{DBLP:journals/neco/HintonOT06} to develop an efficient pre-training process whose results can then be refined using a few iterations of the traditional result. In this portion of the paper we discuss the work of \cite{DBLP:journals/neco/Hinton02} and explain the greedy algorithm of \cite{DBLP:journals/neco/HintonOT06} before going on to discuss the high-level training procedure of \cite{DBLP:journals/taslp/DahlYDA12}.

\paragraph{} To understand the pre-training process, it is necessary to discuss Restricted Boltzmann Machines (RBM) and Deep Belief Networks (DBN). RBMs are an undirected bipartite graphical model with Gaussian distributed input nodes in a visible layer connecting to binary nodes in a hidden layer. Every possible arrangement of hidden, $h$, a visible, $v$, nodes is given a energy under the RBM model: 

\begin{equation}
E(v, h) = - b^T v - c^T h - v^T W h
\end{equation}

Where $W$ is the weight of connections between nodes and vectors $b$ and $c$ correspond to the visible and hidden biases respectively. The resulting probability is then given by:

\begin{equation}
\prob{v, h} = \frac{e^{-E(v, h)}}{Z}
\end{equation}

Where Z is a normalization factor. Based on the assumptions of the RBM, \cite{DBLP:journals/taslp/DahlYDA12} derive expressions for $\cprob{h = 1}{v}$ and $\cprob{v = 1}{h}$ given by:

\begin{equation}
\cprob{h = 1}{v} = \sigma(c + v^T W) \qquad \cprob{v = 1}{h} = \sigma(b + h^T W^T)
\label{eqn:dahlEq7}
\end{equation}

Where $\sigma$ is an element wise logistic function. \cite{DBLP:journals/taslp/DahlYDA12} argue that Eq. (\ref{eqn:dahlEq7}) allows one to repurpose the RBM parameters to initialize a neural network. Training of the RBM is done by stochastic gradient descent against the negative log likelihood:

\begin{equation}
	- \partialDx{\ell(\theta)}{w_{ij}} = \inAngle{ \partialDx{E}{\theta} }_\text{data} - \inAngle{ \partialDx{E}{\theta} }_\text{model}
\end{equation}

however \cite{DBLP:journals/taslp/DahlYDA12} point out that the gradient of the negative log likelihood cannot be computed exactly since the $\inAngle{ \cdot }_\text{model}$ term takes exponential time. As a result, the contrastive divergence method is used to approximate the derivative:

\begin{equation}
	- \partialDx{\ell(\theta)}{w_{ij}} \approx \inAngle{ v_i h_j }_\text{data} - \inAngle{ v_i h_j }_\text{1}
\end{equation}

where $\inAngle{ \cdot }_\text{1}$ is a single step Gibb Sampled expectation. \TODO{What is the underscore data term?; How will you explain in terms of what you read in the product of experts papers?; where's the kullback leiber divergence...? }

\paragraph{} Now that we have an understanding of RBMs, we can shift our focus to DBNs. A Deep belief network are multilayer models with undirected connections between the top two layers and directed between other layers. To train these models, \cite{DBLP:journals/neco/HintonOT06} had the insight to treat adjacent layers of nodes as RBMs. One starts with the bottom two layers and trains them as though they were a single RBMS. Once those two layers are trained, then the top layer is treated as the input layer of a new RBM with the layer above that layer acting is the hidden layer of the new RBMs. The sliding window over the layers continues until the full DBN is trained. \TODO{Ok... so that's good and all, but where's the connection between DBN and DNN?}

\subsection*{Training}

\paragraph{} Training of the CD-DNN-HMM model consists of roughly a dozen involved steps. We won't elaborate here on the full details of each step, but will instead provide a high-level sketch of the procedure to convey its general mechanics. 

\paragraph{} The first high-level step of the procedure is to initialize the CD-DNN-HMM model. This is done by first training a decision tree to find the best tying of triphone states which are then used to train a CD-GMM-HMM system. Next, the unique tied state triphones are each assigned a unique senone identifier. This mapping will then be used to label each of the tied state triphones. (These identifiers will be used later to refine the DNN.) Finally, the trained CD-GMM-HMM is converted into a CD-DNN-HMM by retaining the triphone and senone structure and HMM parameters. This resulting DNN goes through the pre-training procedure discussed in depth earlier. 

\paragraph{} The next high-level step iteratively refines the CD-DNN-HMM. To do this, first the originally trained CD-GMM-HMM model is used to generate a raw alignment of states \TODO{figure out what it's aligning to} which is then mapped to its corresponding senone identifier. This resulting alignment is then used to refine the DBN by backpropagation. Next, the prior senone probability is estimated based on the number of frames \TODO{elaborate more on these frames somewhere} paired with the senone and the total number of frames. These estimates are then used to refine the HMM transition probabilities to maximize the features. Finally, if this newly estimated parameters do not improve accuracy against a development set, then the training procedure terminates; otherwise, the procedure repeats this high-level step.

\paragraph{} \TODO{Discuss computational time to train}

\section*{Convolutional Neural Networks Approach} \TODO{Ryan TODO}

\section*{Experimental Results} 

\subsection*{System Configurations}

\paragraph{} \cite{DBLP:journals/taslp/DahlYDA12} report that their system relies on nationwide language model consisting of 1.5 million trigrams. For their acoustic model, then use a five hidden layer DNN with each layer containing 2,000 hidden units. 

\TODO{Add in CNN details}

\subsection*{Datasets}

\TODO{Garrett}

\subsection*{Results}

\begin{table}[H]
	\centering
	\begin{tabular}{c|c|c|c|c|c}
		& & \multicolumn{2}{c|}{Bing Mobile} & \multicolumn{2}{c}{TIMIT}\\
		& Architecture & Dev. & Test  & Dev. & Test \\
		\hline
		\multirow{2}{*}{Sentence Accuracy} & CD-GMM-HMMM & 70.3\% & 68.4\% & & \\
		& CD-DNN-HMM & 71.8\% & 69.6\% & & \\
		\hline
		Phone Error & CNN-HMM  & & &  & 20.07\%
	\end{tabular}
	\caption{Accuracy and error rates reported by \cite{DBLP:journals/taslp/DahlYDA12} and \cite{DBLP:journals/taslp/Abdel-HamidMJDPY14}.}
	\label{tbl:results}
\end{table}

\paragraph{} Direct comparison of the two systems is complicated by the fact that both papers report different metrics against different datasets. \cite{DBLP:journals/taslp/DahlYDA12} reports a sentence level accuracy rate, while \cite{DBLP:journals/taslp/Abdel-HamidMJDPY14} reports the phone error rate.

\section*{Conclusions} \TODO{Garrett TODO}

\appendix

\singlespacing

\bibliographystyle{alpha}
\bibliography{references}

\end{document}